{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Modelling- LDA and LSA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwJ5IN3SVe3t"
      },
      "source": [
        "# **Topic Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hsgulB7KAi"
      },
      "source": [
        "## **LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0_p0L_KVn8A",
        "outputId": "ec28f1c1-390e-45cf-e782-4ea45b4cca19"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzjxYCXnXBZk",
        "outputId": "aaa9e9e0-93d1-492b-94a8-70c4d6c640c3"
      },
      "source": [
        "print(list(newsgroups_train.target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s858d2lsXyFf"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhzsdJiZX05S",
        "outputId": "6ea3a93f-1005-479f-90ba-ffa1f5d06b17"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep1eysy536e1"
      },
      "source": [
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn0aKWbtYGAd"
      },
      "source": [
        "\n",
        "'''\n",
        "Write a function to perform the pre processing steps on the entire dataset\n",
        "'''\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7fp8UjYX7z4",
        "outputId": "7744cc4b-ec41-4290-db8e-42b3f4a9db10"
      },
      "source": [
        "'''\n",
        "Preview a document after preprocessing\n",
        "'''\n",
        "document_num = 50\n",
        "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
        "\n",
        "print(\"Original document: \")\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print(\"\\n\\nTokenized and lemmatized document: \")\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original document: \n",
            "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
            "\n",
            "\n",
            "Tokenized and lemmatized document: \n",
            "['disk', 'fail', 'time', 'like', 'replac']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhPZKdf44Cof"
      },
      "source": [
        "processed_docs = []\n",
        "\n",
        "for doc in newsgroups_train.data:\n",
        "    processed_docs.append(preprocess(doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "546bMRW6XTZe"
      },
      "source": [
        "import gensim\n",
        "dictionary = gensim.corpora.Dictionary(processed_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXfGfhIFXVF8",
        "outputId": "2881e6cf-bc29-428b-c51a-d896fe75bad6"
      },
      "source": [
        "'''\n",
        "Checking dictionary created\n",
        "'''\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 addit\n",
            "1 bodi\n",
            "2 bricklin\n",
            "3 bring\n",
            "4 bumper\n",
            "5 call\n",
            "6 colleg\n",
            "7 door\n",
            "8 earli\n",
            "9 engin\n",
            "10 enlighten\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw9g4tGA4hfd"
      },
      "source": [
        "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaLeiqbQ6JD6"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTM85KtT6K5o",
        "outputId": "751f560f-3292-4ded-fab9-5b3210ad51df"
      },
      "source": [
        "document_num = 20\n",
        "bow_doc_x = bow_corpus[document_num]\n",
        "\n",
        "for i in range(len(bow_doc_x)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
        "                                                     dictionary[bow_doc_x[i][0]], \n",
        "                                                     bow_doc_x[i][1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 18 (\"rest\") appears 1 time.\n",
            "Word 166 (\"clear\") appears 1 time.\n",
            "Word 336 (\"refer\") appears 1 time.\n",
            "Word 350 (\"true\") appears 1 time.\n",
            "Word 391 (\"technolog\") appears 1 time.\n",
            "Word 437 (\"christian\") appears 1 time.\n",
            "Word 453 (\"exampl\") appears 1 time.\n",
            "Word 476 (\"jew\") appears 1 time.\n",
            "Word 480 (\"lead\") appears 1 time.\n",
            "Word 482 (\"littl\") appears 3 time.\n",
            "Word 520 (\"wors\") appears 2 time.\n",
            "Word 721 (\"keith\") appears 3 time.\n",
            "Word 732 (\"punish\") appears 1 time.\n",
            "Word 803 (\"california\") appears 1 time.\n",
            "Word 859 (\"institut\") appears 1 time.\n",
            "Word 917 (\"similar\") appears 1 time.\n",
            "Word 990 (\"allan\") appears 1 time.\n",
            "Word 991 (\"anti\") appears 1 time.\n",
            "Word 992 (\"arriv\") appears 1 time.\n",
            "Word 993 (\"austria\") appears 1 time.\n",
            "Word 994 (\"caltech\") appears 2 time.\n",
            "Word 995 (\"distinguish\") appears 1 time.\n",
            "Word 996 (\"german\") appears 1 time.\n",
            "Word 997 (\"germani\") appears 3 time.\n",
            "Word 998 (\"hitler\") appears 1 time.\n",
            "Word 999 (\"livesey\") appears 2 time.\n",
            "Word 1000 (\"motto\") appears 2 time.\n",
            "Word 1001 (\"order\") appears 1 time.\n",
            "Word 1002 (\"pasadena\") appears 1 time.\n",
            "Word 1003 (\"pompous\") appears 1 time.\n",
            "Word 1004 (\"popul\") appears 1 time.\n",
            "Word 1005 (\"rank\") appears 1 time.\n",
            "Word 1006 (\"schneider\") appears 1 time.\n",
            "Word 1007 (\"semit\") appears 1 time.\n",
            "Word 1008 (\"social\") appears 1 time.\n",
            "Word 1009 (\"solntz\") appears 1 time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC_Ie2VG6Nrt"
      },
      "source": [
        "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 8, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGUpgVIL6UXQ",
        "outputId": "a99a38a6-d588-4d85-a4ef-c6d18fe1a4c7"
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.012*\"armenian\" + 0.011*\"game\" + 0.010*\"team\" + 0.006*\"turkish\" + 0.006*\"play\" + 0.006*\"player\" + 0.005*\"season\" + 0.004*\"leagu\" + 0.004*\"leav\" + 0.004*\"basebal\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.017*\"drive\" + 0.006*\"control\" + 0.005*\"hard\" + 0.005*\"disk\" + 0.004*\"firearm\" + 0.004*\"bike\" + 0.004*\"caus\" + 0.004*\"effect\" + 0.003*\"car\" + 0.003*\"weapon\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.011*\"scsi\" + 0.009*\"wire\" + 0.008*\"power\" + 0.006*\"appl\" + 0.005*\"control\" + 0.005*\"chip\" + 0.005*\"speed\" + 0.005*\"data\" + 0.005*\"connect\" + 0.004*\"circuit\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.010*\"space\" + 0.009*\"nasa\" + 0.005*\"presid\" + 0.005*\"nation\" + 0.004*\"research\" + 0.004*\"program\" + 0.004*\"center\" + 0.004*\"report\" + 0.004*\"health\" + 0.004*\"group\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.011*\"govern\" + 0.009*\"encrypt\" + 0.008*\"secur\" + 0.007*\"chip\" + 0.007*\"israel\" + 0.006*\"clipper\" + 0.006*\"public\" + 0.006*\"isra\" + 0.005*\"protect\" + 0.004*\"key\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.009*\"game\" + 0.008*\"toronto\" + 0.008*\"play\" + 0.007*\"team\" + 0.007*\"hockey\" + 0.006*\"canada\" + 0.005*\"player\" + 0.005*\"columbia\" + 0.004*\"henri\" + 0.004*\"playoff\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.015*\"window\" + 0.014*\"file\" + 0.009*\"program\" + 0.007*\"card\" + 0.006*\"version\" + 0.006*\"softwar\" + 0.006*\"imag\" + 0.005*\"graphic\" + 0.005*\"avail\" + 0.005*\"color\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.010*\"christian\" + 0.006*\"jesus\" + 0.006*\"exist\" + 0.004*\"claim\" + 0.004*\"moral\" + 0.004*\"human\" + 0.004*\"jew\" + 0.004*\"word\" + 0.004*\"religion\" + 0.004*\"bibl\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfI560TP6YBK",
        "outputId": "41f801fb-6884-40e9-8246-c2cda0f39262"
      },
      "source": [
        "num = 100\n",
        "unseen_document = newsgroups_test.data[num]\n",
        "print(unseen_document)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: help\n",
            "From: C..Doelle@p26.f3333.n106.z1.fidonet.org (C. Doelle)\n",
            "Lines: 13\n",
            "\n",
            "Hello All!\n",
            "\n",
            "    It is my understanding that all True-Type fonts in Windows are loaded in\n",
            "prior to starting Windows - this makes getting into Windows quite slow if you\n",
            "have hundreds of them as I do.  First off, am I correct in this thinking -\n",
            "secondly, if that is the case - can you get Windows to ignore them on boot and\n",
            "maybe make something like a PIF file to load them only when you enter the\n",
            "applications that need fonts?  Any ideas?\n",
            "\n",
            "\n",
            "Chris\n",
            "\n",
            " * Origin: chris.doelle.@f3333.n106.z1.fidonet.org (1:106/3333.26)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bGudfo_6d9M",
        "outputId": "5e085476-5caf-47a1-a38e-21e59b2915aa"
      },
      "source": [
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.9717352986335754\t Topic: 0.015*\"window\" + 0.014*\"file\" + 0.009*\"program\" + 0.007*\"card\" + 0.006*\"version\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj9X2IqR6e6C",
        "outputId": "fe105856-258e-4a48-ab1d-10ce966a746f"
      },
      "source": [
        "print(newsgroups_test.target[num])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeKxGkEC6y_e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81oPg4_7NqS"
      },
      "source": [
        "## **LSA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sen4Ipt_T10T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f077f440-f19b-4fd8-c4aa-bed54040bffd"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Reviews.csv', nrows = 10000)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SXFN2bg9VFT",
        "outputId": "706a498f-d9bd-495a-c570-ca8176d78f1a"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit(df['Text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "GnFsfyhf9VDD",
        "outputId": "7d669c83-17c1-42ef-d52e-30e148d74c91"
      },
      "source": [
        "X = tfidf.transform(df['Text'])\n",
        "df['Text'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59zfPKLb9VBO",
        "outputId": "502dc808-12dd-4e91-d85e-bd9d1c4da56a"
      },
      "source": [
        "print([X[1, tfidf.vocabulary_['peanuts']]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.396726111335369]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ZSjVcyvl9U95",
        "outputId": "26536d95-13f2-43ef-cdf7-e7a0939b8bc5"
      },
      "source": [
        "import numpy as np\n",
        "df.dropna(inplace=True)\n",
        "df[df['Score'] != 3]\n",
        "df['Positivity'] = np.where(df['Score'] > 3, 1, 0)\n",
        "cols = ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary']\n",
        "df.drop(cols, axis=1, inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Positivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Positivity\n",
              "0  I have bought several of the Vitality canned d...           1\n",
              "1  Product arrived labeled as Jumbo Salted Peanut...           0\n",
              "2  This is a confection that has been around a fe...           1\n",
              "3  If you are looking for the secret ingredient i...           0\n",
              "4  Great taffy at a great price.  There was a wid...           1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEqOXOkG9U7F",
        "outputId": "72e3bf59-c272-4ecc-b96c-609fd5933996"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.Text\n",
        "y = df.Positivity\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_train),\n",
        "                                                                             (len(X_train[y_train == 0]) / (len(X_train)*1.))*100,\n",
        "                                                                            (len(X_train[y_train == 1]) / (len(X_train)*1.))*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set has total 7500 entries with 23.57% negative, 76.43% positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p01KvVFB9U4S",
        "outputId": "ab48d855-ebfd-4d40-8717-d44d7d7c49f9"
      },
      "source": [
        "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_test),\n",
        "                                                                             (len(X_test[y_test == 0]) / (len(X_test)*1.))*100,\n",
        "                                                                            (len(X_test[y_test == 1]) / (len(X_test)*1.))*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set has total 2500 entries with 24.64% negative, 75.36% positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD0UtNq49iNs"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "def accuracy_summary(pipeline, X_train, y_train, X_test, y_test):\n",
        "    sentiment_fit = pipeline.fit(X_train, y_train)\n",
        "    y_pred = sentiment_fit.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPLJK_029iLd",
        "outputId": "b6652151-4c36-480d-ce1a-141a68387341"
      },
      "source": [
        "cv = CountVectorizer()\n",
        "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
        "n_features = np.arange(10000,30001,10000)\n",
        "def nfeature_accuracy_checker(vectorizer=cv, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=rf):\n",
        "    result = []\n",
        "    print(classifier)\n",
        "    print(\"\\n\")\n",
        "    for n in n_features:\n",
        "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
        "        checker_pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', classifier)\n",
        "        ])\n",
        "        print(\"Test result for {} features\".format(n))\n",
        "        nfeature_accuracy = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)\n",
        "        result.append((n,nfeature_accuracy))\n",
        "    return result\n",
        "tfidf = TfidfVectorizer()\n",
        "print(\"Result for trigram with stop words (Tfidf)\\n\")\n",
        "feature_result_tgt = nfeature_accuracy_checker(vectorizer=tfidf,ngram_range=(1, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for trigram with stop words (Tfidf)\n",
            "\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "\n",
            "\n",
            "Test result for 10000 features\n",
            "accuracy score: 81.72%\n",
            "Test result for 20000 features\n",
            "accuracy score: 81.96%\n",
            "Test result for 30000 features\n",
            "accuracy score: 81.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuqC4gSX9iJK",
        "outputId": "9cf7fd9d-9e6f-47e8-8771-e40edb4bd75f"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "cv = CountVectorizer(max_features=30000,ngram_range=(1, 3))\n",
        "pipeline = Pipeline([\n",
        "        ('vectorizer', cv),\n",
        "        ('classifier', rf)\n",
        "    ])\n",
        "sentiment_fit = pipeline.fit(X_train, y_train)\n",
        "y_pred = sentiment_fit.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=['negative','positive']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.93      0.34      0.50       616\n",
            "    positive       0.82      0.99      0.90      1884\n",
            "\n",
            "    accuracy                           0.83      2500\n",
            "   macro avg       0.87      0.67      0.70      2500\n",
            "weighted avg       0.85      0.83      0.80      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMtBFwTo9UvT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}